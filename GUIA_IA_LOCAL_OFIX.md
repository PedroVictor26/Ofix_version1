# ü§ñ Guia de Instala√ß√£o e Configura√ß√£o da IA Local - OFIX

## üìã Resumo Executivo

**Problema resolvido:** Depend√™ncia de APIs pagas (OpenAI/Claude) e falta de controle sobre dados sens√≠veis.

**Solu√ß√£o implementada:** Sistema modular de IA que suporta m√∫ltiplos provedores:
- **Ollama** (local - RECOMENDADO) üè†
- **Hugging Face** (gratuito online) ü§ó  
- **OpenAI** (pago - para compara√ß√£o) üí∞
- **Modelos Customizados** (treinados especificamente para oficina) üéØ

## üöÄ Instala√ß√£o R√°pida (15 minutos)

### 1. **Instalar Ollama (IA Local)**

```powershell
# Baixar e instalar Ollama
# Ir para: https://ollama.ai/download
# Ou usar winget:
winget install Ollama.Ollama

# Verificar instala√ß√£o
ollama --version

# Instalar modelo principal (4.7GB)
ollama pull llama3.1:8b

# Instalar modelo para embeddings (274MB)
ollama pull nomic-embed-text
```

### 2. **Configurar Backend OFIX**

```bash
# Ir para pasta do backend
cd "c:\OfixNovo - Copia\ofix_new\ofix-backend"

# Copiar configura√ß√µes
cp .env.example .env

# Editar .env com estas configura√ß√µes:
```

**Arquivo `.env`:**
```env
# ===== CONFIGURA√á√ïES DE IA =====
AI_PROVIDER="ollama"
OLLAMA_HOST="http://localhost:11434"
OLLAMA_MODEL="llama3.1:8b"

# Opcional - para testes comparativos
HUGGINGFACE_TOKEN="hf_seu_token_aqui"
OPENAI_API_KEY="sk_sua_chave_aqui"
```

### 3. **Iniciar Sistema**

```powershell
# Terminal 1: Iniciar Ollama (se n√£o iniciou automaticamente)
ollama serve

# Terminal 2: Iniciar backend
cd "c:\OfixNovo - Copia\ofix_new\ofix-backend"
npm start

# Terminal 3: Iniciar frontend
cd "c:\OfixNovo - Copia\ofix_new"
npm run dev
```

## üß™ Testar Funcionalidades

### 1. **Verificar Status dos Provedores**
```bash
curl http://localhost:1000/api/ai/providers/status
```

### 2. **Testar Triagem por Voz**
```bash
# Upload de arquivo de √°udio para triagem
curl -X POST -F "audio=@exemplo.wav" \
  http://localhost:1000/api/ai/triagem-voz
```

### 3. **Comparar Provedores**
```bash
curl -X POST http://localhost:1000/api/ai/providers/compare \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Como diagnosticar problema de freios?"}'
```

## üìä Vantagens da IA Local vs APIs Pagas

| Crit√©rio | IA Local (Ollama) | OpenAI/Claude |
|----------|-------------------|---------------|
| **Custo** | ‚úÖ Gratuito | ‚ùå $20-100/m√™s |
| **Privacidade** | ‚úÖ Dados locais | ‚ùå Dados na nuvem |
| **Velocidade** | ‚úÖ Instant | ‚ö†Ô∏è Depende da internet |
| **Customiza√ß√£o** | ‚úÖ Treino pr√≥prio | ‚ùå Limitado |
| **Depend√™ncia** | ‚úÖ Independente | ‚ùå Internet + $$ |
| **Dados oficina** | ‚úÖ Pode treinar | ‚ùå Gen√©rico |

## üéØ Funcionalidades Implementadas

### 1. **Triagem por Voz com IA Local**
- ‚úÖ Transcri√ß√£o de √°udio (Whisper local ou HF)
- ‚úÖ An√°lise autom√°tica de problemas
- ‚úÖ Categoriza√ß√£o (motor, freios, etc.)
- ‚úÖ Estimativa de tempo e complexidade
- ‚úÖ Sugest√£o de pe√ßas

### 2. **Sistema Modular de Provedores**
- ‚úÖ Ollama (local)
- ‚úÖ Hugging Face (gratuito)
- ‚úÖ OpenAI (pago)
- ‚úÖ Modelos customizados
- ‚úÖ Fallback autom√°tico

### 3. **Dashboards de IA**
- ‚úÖ Status dos provedores
- ‚úÖ Compara√ß√£o de performance
- ‚úÖ Instala√ß√£o de modelos
- ‚úÖ M√©tricas de uso

## üîß Configura√ß√£o Avan√ßada

### **Hugging Face (Alternativa Gratuita)**

1. **Criar conta gratuita:** https://huggingface.co
2. **Gerar token:** Settings ‚Üí Access Tokens
3. **Configurar no .env:**
```env
HUGGINGFACE_TOKEN="hf_seu_token_aqui"
HF_MODEL="microsoft/DialoGPT-medium"
```

### **Modelos Customizados (Treinamento Pr√≥prio)**

1. **Preparar dados de treinamento:**
```json
[
  {
    "input": "O freio est√° rangendo",
    "output": "freios",
    "categories": ["motor", "freios", "suspensao", "eletrica"]
  }
]
```

2. **Iniciar treinamento:**
```bash
curl -X POST http://localhost:1000/api/ai/training/start \
  -H "Content-Type: application/json" \
  -d '{
    "modelName": "ofix-triagem-v1",
    "trainingData": [...],
    "config": {"epochs": 50}
  }'
```

## üéÆ Interface de Gest√£o da IA

Criar componente React para gerenciar IA:

```jsx
// src/components/ai/AIProviderManager.jsx
const AIProviderManager = () => {
  const [providers, setProviders] = useState({});
  const [currentProvider, setCurrentProvider] = useState('ollama');
  
  useEffect(() => {
    fetch('/api/ai/providers/status')
      .then(r => r.json())
      .then(data => {
        setProviders(data.providers);
        setCurrentProvider(data.currentProvider);
      });
  }, []);
  
  return (
    <div className="ai-manager">
      <h2>ü§ñ Gest√£o de IA</h2>
      
      {/* Status dos provedores */}
      <div className="providers-grid">
        {Object.entries(providers).map(([name, info]) => (
          <div key={name} className={`provider-card ${info.available ? 'available' : 'unavailable'}`}>
            <h3>{name}</h3>
            <p>{info.description}</p>
            <span className={`status ${info.available ? 'online' : 'offline'}`}>
              {info.available ? '‚úÖ Online' : '‚ùå Offline'}
            </span>
          </div>
        ))}
      </div>
      
      {/* Teste r√°pido */}
      <div className="quick-test">
        <h3>üß™ Teste R√°pido</h3>
        <button onClick={testCurrentProvider}>
          Testar {currentProvider}
        </button>
      </div>
    </div>
  );
};
```

## üìà Pr√≥ximos Passos

### **Fase 1: Valida√ß√£o (Esta semana)**
- [x] Instalar Ollama
- [x] Configurar sistema modular  
- [x] Testar triagem b√°sica
- [ ] Validar com dados reais

### **Fase 2: Treinamento (Pr√≥ximas 2 semanas)**
- [ ] Coletar dados hist√≥ricos da oficina
- [ ] Treinar modelo espec√≠fico para triagem
- [ ] Fine-tuning para mensagens WhatsApp
- [ ] Modelo para alertas inteligentes

### **Fase 3: Produ√ß√£o (M√™s 1)**
- [ ] M√©tricas de performance
- [ ] Sistema de feedback cont√≠nuo
- [ ] Auto-aprendizado baseado nos casos
- [ ] Deploy em produ√ß√£o

## üîç Troubleshooting

### **Ollama n√£o inicia**
```powershell
# Verificar se est√° rodando
ollama list

# Reiniciar servi√ßo
ollama serve

# Verificar porta
netstat -an | findstr 11434
```

### **Modelo n√£o responde**
```bash
# Reinstalar modelo
ollama rm llama3.1:8b
ollama pull llama3.1:8b

# Testar diretamente
ollama run llama3.1:8b "Diga ol√°"
```

### **Performance lenta**
- **RAM:** M√≠nimo 8GB, recomendado 16GB
- **CPU:** Qualquer processador moderno
- **Modelo menor:** Use `llama3.1:3b` em vez de `8b`

## üìû Suporte

**Problemas t√©cnicos:**
1. Verificar logs: `console.log` no backend
2. Testar endpoint: `/api/ai/health`
3. Validar .env: Todas vari√°veis configuradas

**Performance:**
- Ollama local: 2-5 segundos
- Hugging Face: 5-15 segundos  
- OpenAI: 1-3 segundos

---

**‚ú® Resultado:** IA pr√≥pria funcionando 100% local, sem custos mensais, com dados protegidos e performance excelente!

**Data:** 03 de Setembro de 2025
**Vers√£o:** 1.0 - Sistema implementado e funcionando
