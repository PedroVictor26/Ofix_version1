/**
 * Provedor Ollama para IA Local
 * Executa modelos LLM localmente sem depend√™ncias externas
 */

import { Ollama } from 'ollama';

class OllamaProvider {
  constructor() {
    this.client = new Ollama({
      host: process.env.OLLAMA_HOST || 'http://localhost:11434'
    });
    this.defaultModel = process.env.OLLAMA_MODEL || 'llama3.1:8b';
    this.whisperModel = 'whisper:latest';
  }

  async isAvailable() {
    try {
      await this.client.list();
      return true;
    } catch (error) {
      console.warn('ü¶ô Ollama n√£o dispon√≠vel:', error.message);
      return false;
    }
  }

  async generateText(prompt, options = {}) {
    const response = await this.client.generate({
      model: options.model || this.defaultModel,
      prompt,
      stream: false,
      options: {
        temperature: options.temperature || 0.7,
        max_tokens: options.maxTokens || 1000,
        top_p: options.topP || 0.9
      }
    });

    return {
      text: response.response,
      model: response.model,
      usage: {
        promptTokens: response.prompt_eval_count || 0,
        completionTokens: response.eval_count || 0,
        totalTokens: (response.prompt_eval_count || 0) + (response.eval_count || 0)
      }
    };
  }

  async generateStructuredResponse(prompt, schema, options = {}) {
    const structuredPrompt = `${prompt}

Responda APENAS em formato JSON v√°lido seguindo este schema:
${JSON.stringify(schema, null, 2)}

Sua resposta deve ser um JSON v√°lido, sem texto adicional:`;

    const response = await this.generateText(structuredPrompt, {
      ...options,
      temperature: 0.3 // Menor temperatura para respostas mais consistentes
    });

    try {
      return JSON.parse(response.text);
    } catch (error) {
      // Tentar extrair JSON da resposta
      const jsonMatch = response.text.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        return JSON.parse(jsonMatch[0]);
      }
      throw new Error('Resposta n√£o est√° em formato JSON v√°lido');
    }
  }

  async transcribeAudio(audioBuffer, options = {}) {
    // Para transcri√ß√£o local, seria necess√°rio usar whisper.cpp ou similar
    // Por enquanto, vamos simular com um placeholder
    throw new Error('Transcri√ß√£o de √°udio local ainda n√£o implementada. Use OpenAI para esta funcionalidade.');
  }

  // M√©todo espec√≠fico para triagem automotiva
  async analisarTriagemVeiculo(transcricao, options = {}) {
    const prompt = `Voc√™ √© um especialista em diagn√≥stico automotivo. Analise a descri√ß√£o do problema e retorne uma an√°lise estruturada.

DESCRI√á√ÉO DO CLIENTE: "${transcricao}"

Analise e categorize o problema seguindo estas regras:
- Seja espec√≠fico e t√©cnico
- Base-se em conhecimento automotivo real
- Considere a urg√™ncia do problema
- Sugira pr√≥ximos passos pr√°ticos

Responda em JSON com esta estrutura exata:`;

    const schema = {
      categoria_principal: "string (ex: motor, freios, suspensao, eletrica, transmissao)",
      categoria_secundaria: "string (mais espec√≠fico)",
      tempo_estimado_horas: "number (1-40)",
      complexidade: "string (baixa|media|alta)",
      urgencia: "string (baixa|media|alta|emergencia)",
      pecas_provaveis: ["array de strings"],
      proximos_passos: "string (a√ß√£o espec√≠fica para o mec√¢nico)",
      confianca_analise: "number (0-100)",
      sintomas_identificados: ["array de strings"],
      diagnostico_preliminar: "string"
    };

    return await this.generateStructuredResponse(prompt, schema, {
      temperature: 0.4,
      maxTokens: 800
    });
  }

  // M√©todo para gera√ß√£o de mensagens WhatsApp
  async gerarMensagemWhatsApp(servico, statusNovo, statusAnterior) {
    const prompt = `Gere uma mensagem WhatsApp profissional e amig√°vel para cliente de oficina mec√¢nica.

DADOS DO SERVI√áO:
- Cliente: ${servico.cliente_nome}
- Ve√≠culo: ${servico.veiculo_modelo} ${servico.veiculo_ano || ''}
- Status anterior: ${statusAnterior}
- Status novo: ${statusNovo}
- Mec√¢nico: ${servico.mecanico_nome || 'Nossa equipe'}
- Prazo estimado: ${servico.prazo_estimado || 'Em breve'}

REGRAS:
- M√°ximo 160 caracteres
- Tom profissional mas amig√°vel
- Inclua emoji apropriado
- Seja espec√≠fico sobre o status
- Tranquilize o cliente

Responda apenas com a mensagem, sem aspas ou formata√ß√£o adicional:`;

    const response = await this.generateText(prompt, {
      temperature: 0.6,
      maxTokens: 100
    });

    return response.text.trim();
  }

  // M√©todo para alertas inteligentes
  async analisarAlertaPrazo(servico, horasAtraso) {
    const prompt = `Analise este servi√ßo automotivo em atraso e sugira 3 a√ß√µes espec√≠ficas e pr√°ticas.

SITUA√á√ÉO:
- Servi√ßo: ${servico.descricao}
- Cliente: ${servico.cliente_nome}
- Mec√¢nico: ${servico.mecanico?.nome || 'N√£o alocado'}
- Status: ${servico.status}
- Horas de atraso: ${horasAtraso}

Voc√™ deve sugerir a√ß√µes ESPEC√çFICAS e PR√ÅTICAS que podem ser tomadas imediatamente.

Responda em JSON:`;

    const schema = {
      prioridade: "string (baixa|media|alta|critica)",
      resumo: "string (resumo da situa√ß√£o em 1 linha)",
      acoes: [
        "string (a√ß√£o espec√≠fica 1)",
        "string (a√ß√£o espec√≠fica 2)", 
        "string (a√ß√£o espec√≠fica 3)"
      ],
      impacto_cliente: "string (como isso afeta o cliente)",
      tempo_resolucao_estimado: "string (ex: '30 minutos', '2 horas')"
    };

    return await this.generateStructuredResponse(prompt, schema, {
      temperature: 0.3,
      maxTokens: 500
    });
  }

  // M√©todo para embeddings (usando modelo local se dispon√≠vel)
  async generateEmbedding(text) {
    try {
      const response = await this.client.embeddings({
        model: 'nomic-embed-text:latest',
        prompt: text
      });
      return response.embedding;
    } catch (error) {
      console.warn('‚ùå Embedding local n√£o dispon√≠vel:', error.message);
      throw new Error('Embedding local n√£o configurado');
    }
  }

  // M√©todo para instalar modelos automaticamente
  async installModel(modelName) {
    try {
      console.log(`üì• Baixando modelo ${modelName}...`);
      await this.client.pull({ model: modelName, stream: false });
      console.log(`‚úÖ Modelo ${modelName} instalado com sucesso`);
      return true;
    } catch (error) {
      console.error(`‚ùå Erro ao instalar modelo ${modelName}:`, error.message);
      return false;
    }
  }

  // M√©todo para listar modelos instalados
  async listInstalledModels() {
    try {
      const response = await this.client.list();
      return response.models.map(model => ({
        name: model.name,
        size: model.size,
        modified: model.modified_at
      }));
    } catch (error) {
      console.error('‚ùå Erro ao listar modelos:', error.message);
      return [];
    }
  }
}

export default OllamaProvider;
